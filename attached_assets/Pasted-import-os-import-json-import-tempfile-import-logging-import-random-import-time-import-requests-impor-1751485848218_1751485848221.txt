import os
import json
import tempfile
import logging
import random
import time
import requests
import hashlib
import base64
from urllib.parse import urlparse
from flask import Flask, request, send_file, jsonify
from flask_cors import CORS
import genanki

# Configure logging
logging.basicConfig(level=logging.DEBUG)

# Create the app
app = Flask(__name__)
app.secret_key = os.environ.get("SESSION_SECRET", "dev-secret-key-change-in-production")

# Configure CORS for API endpoints
CORS(app, resources={
    r"/api/*": {
        "origins": "*",
        "methods": ["GET", "POST", "OPTIONS"],
        "allow_headers": ["Content-Type", "Authorization"]
    }
})

# Global storage for base64 images during processing
IMAGE_STORE = {}

def store_base64_image(base64_string, image_ref):
    """Store base64 image and return a short reference"""
    # Remove data URI prefix if present
    if base64_string.startswith('data:'):
        base64_string = base64_string.split(',')[1]
    
    # Store the full base64 string
    IMAGE_STORE[image_ref] = base64_string
    return image_ref

def save_base64_as_file(image_ref, media_files_list):
    """Convert stored base64 image to file for Anki"""
    if image_ref not in IMAGE_STORE:
        return None
    
    try:
        base64_string = IMAGE_STORE[image_ref]
        image_data = base64.b64decode(base64_string)
        
        # Create filename
        filename = f"{image_ref}.jpg"
        temp_path = os.path.join(tempfile.gettempdir(), filename)
        
        # Write to file
        with open(temp_path, 'wb') as f:
            f.write(image_data)
        
        media_files_list.append(temp_path)
        return filename
    except Exception as e:
        app.logger.error(f"Error saving base64 image {image_ref}: {e}")
        return None

def process_pdf_data(data):
    """Process PDF data into organized format for flashcard generation"""
    
    # Initialize organized data structure
    organized_data = {
        "metadata": {
            "filename": data.get("document_info", {}).get("filename", "Unknown"),
            "pages": data.get("document_info", {}).get("pages", 0),
            "mainTopic": "Medicine",  # Default topic
            "uniformTag": "Medicine::Cards",
            "processingTime": data.get("processing_time", 0)
        },
        "objectives": data.get("session_objectives", []),
        "content": {
            "critical": [],
            "highYield": [],
            "testable": []
        },
        "images": [],
        "stats": {
            "totalContent": 0,
            "filteredContent": 0,
            "criticalItems": 0,
            "highYieldItems": 0,
            "testableItems": 0,
            "totalImages": 0,
            "clinicalImages": 0
        }
    }
    
    # Process content chunks
    content_items = data.get("content", [])
    
    for idx, chunk in enumerate(content_items):
        # Create organized chunk
        organized_chunk = {
            "index": idx,
            "page": chunk.get("page", 1),
            "text": chunk.get("text", ""),
            "html_formatted_text": chunk.get("html_formatted_text", ""),
            "is_important": chunk.get("is_important", False),
            "is_critical_warning": chunk.get("is_critical_warning", False),
            "high_yield_score": chunk.get("high_yield_score", 0),
            "formatting": chunk.get("formatting", {}),
            "testable_facts": chunk.get("testable_facts", []),
            "medical_category": chunk.get("medical_category", "General"),
            "images": []  # Will be populated with image refs for this chunk
        }
        
        # Categorize content
        if chunk.get("is_critical_warning", False):
            organized_data["content"]["critical"].append(organized_chunk)
            organized_data["stats"]["criticalItems"] += 1
        elif chunk.get("is_important", False) or chunk.get("high_yield_score", 0) > 0.7:
            organized_data["content"]["highYield"].append(organized_chunk)
            organized_data["stats"]["highYieldItems"] += 1
        elif len(chunk.get("testable_facts", [])) > 0:
            organized_data["content"]["testable"].append(organized_chunk)
            organized_data["stats"]["testableItems"] += 1
        
        organized_data["stats"]["totalContent"] += 1
    
    # Process images
    images = data.get("images", [])
    for idx, image in enumerate(images):
        image_ref = f"IMG_{idx}"
        
        # Handle base64 images
        if isinstance(image, dict) and "base64" in image:
            store_base64_image(image["base64"], image_ref)
            image_url = image_ref  # Use reference instead of full base64
        elif isinstance(image, str) and image.startswith("data:"):
            store_base64_image(image, image_ref)
            image_url = image_ref
        else:
            image_url = image.get("url", "") if isinstance(image, dict) else str(image)
        
        # Create organized image data
        organized_image = {
            "ref": image_ref,
            "page": image.get("page", 1) if isinstance(image, dict) else 1,
            "description": image.get("description", f"Image from page {image.get('page', 1)}") if isinstance(image, dict) else f"Image {idx + 1}",
            "isSlide": image.get("is_slide", False) if isinstance(image, dict) else False,
            "isClinical": image.get("is_clinical", False) if isinstance(image, dict) else False,
            "useForPage": image.get("page", 1) if isinstance(image, dict) else 1
        }
        
        organized_data["images"].append(organized_image)
        organized_data["stats"]["totalImages"] += 1
        
        if organized_image["isClinical"]:
            organized_data["stats"]["clinicalImages"] += 1
        
        # Link images to content chunks on the same page
        for content_type in ["critical", "highYield", "testable"]:
            for chunk in organized_data["content"][content_type]:
                if chunk["page"] == organized_image["useForPage"]:
                    chunk["images"].append(image_ref)
    
    # Add processing instructions
    organized_data["instructions"] = {
        "createMinimum": 10,
        "createMaximum": 20,
        "priorityOrder": ["critical", "highYield", "objectives", "testable"],
        "imageUsage": "Use image references (e.g., IMG_0) for cards. These will be replaced with actual URLs later.",
        "uniformTag": f"All cards MUST be tagged with: [\"{organized_data['metadata']['uniformTag']}\"]"
    }
    
    return organized_data

def generate_flashcard_prompt(organized_data):
    """Generate a comprehensive prompt for the LLM"""
    
    prompt_parts = []
    
    # Header
    prompt_parts.append(f"Process this medical education content into Anki flashcards:\n")
    prompt_parts.append(f"DOCUMENT: {organized_data['metadata']['filename']} ({organized_data['metadata']['pages']} pages)")
    prompt_parts.append(f"MAIN TOPIC: {organized_data['metadata']['mainTopic']}")
    prompt_parts.append(f"UNIFORM TAG: {organized_data['metadata']['uniformTag']}\n")
    
    # Learning objectives
    if organized_data['objectives']:
        prompt_parts.append("LEARNING OBJECTIVES:")
        for obj in organized_data['objectives']:
            prompt_parts.append(f"- {obj}")
        prompt_parts.append("")
    
    # Content sections
    content_types = [
        ("CRITICAL", "critical"),
        ("HIGH-YIELD", "highYield"),
        ("TESTABLE", "testable")
    ]
    
    for label, key in content_types:
        items = organized_data['content'][key]
        prompt_parts.append(f"\n{label} CONTENT ({len(items)} items):")
        
        if items:
            for item in items[:5]:  # Show first 5 items
                prompt_parts.append(f"\nPage {item['page']}:")
                prompt_parts.append(f"HTML: {item['html_formatted_text'][:200]}...")
                if item['testable_facts']:
                    prompt_parts.append(f"Facts: {', '.join(item['testable_facts'][:3])}")
                if item['images']:
                    prompt_parts.append(f"Images: {', '.join(item['images'])}")
            
            if len(items) > 5:
                prompt_parts.append(f"... and {len(items) - 5} more items")
        prompt_parts.append("")
    
    # Images summary
    prompt_parts.append(f"\nAVAILABLE IMAGES ({organized_data['stats']['totalImages']}):")
    for img in organized_data['images'][:10]:  # Show first 10 images
        clinical_tag = " [CLINICAL]" if img['isClinical'] else ""
        prompt_parts.append(f"- {img['ref']}: Page {img['page']} - {img['description']}{clinical_tag}")
    
    if len(organized_data['images']) > 10:
        prompt_parts.append(f"... and {len(organized_data['images']) - 10} more images")
    
    prompt_parts.append(f"\nCreate {organized_data['instructions']['createMinimum']}-{organized_data['instructions']['createMaximum']} flashcards following the exact HTML formatting requirements.")
    
    return "\n".join(prompt_parts)

@app.route('/api/process-pdf', methods=['POST'])
def process_pdf():
    """New endpoint that processes PDF data and returns organized structure"""
    try:
        data = request.get_json(force=True)
        
        # Process and organize the data
        organized_data = process_pdf_data(data)
        
        # Generate the prompt
        flashcard_prompt = generate_flashcard_prompt(organized_data)
        
        # Create response with all organized data
        response = {
            "status": "success",
            "organized_data": organized_data,
            "flashcard_prompt": flashcard_prompt,
            "image_store": {
                "total_stored": len(IMAGE_STORE),
                "references": list(IMAGE_STORE.keys())
            }
        }
        
        return jsonify(response), 200
        
    except Exception as e:
        app.logger.error(f"Error processing PDF data: {e}")
        import traceback
        return jsonify({
            "status": "error",
            "message": str(e),
            "traceback": traceback.format_exc()
        }), 500

def download_image_from_url(url, media_files_list):
    """Download image from URL and return local filename for Anki embedding"""
    # Check if this is an image reference
    if url in IMAGE_STORE:
        return save_base64_as_file(url, media_files_list)
    
    # Original URL download logic
    try:
        parsed_url = urlparse(url)
        filename = os.path.basename(parsed_url.path)
        if not filename or '.' not in filename:
            url_hash = hashlib.md5(url.encode()).hexdigest()[:8]
            filename = f"image_{url_hash}.jpg"

        valid_extensions = ['.jpg', '.jpeg', '.png', '.gif', '.svg', '.webp']
        if not any(filename.lower().endswith(ext) for ext in valid_extensions):
            filename += '.jpg'

        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }

        response = requests.get(url, headers=headers, timeout=30)
        response.raise_for_status()

        temp_path = os.path.join(tempfile.gettempdir(), filename)
        with open(temp_path, 'wb') as f:
            f.write(response.content)

        media_files_list.append(temp_path)
        return filename
    except Exception as e:
        app.logger.error(f"Error downloading image from {url}: {e}")
        return None

# ... (rest of your existing code for create_enhanced_medical_model, EnhancedFlashcardProcessor, etc.)

@app.route('/api/debug-input', methods=['POST'])
def debug_input():
    """Debug endpoint to see what data is being received"""
    try:
        data = request.get_json(force=True)
        
        # Log the structure
        app.logger.info("=== DEBUG INPUT ===")
        app.logger.info(f"Data type: {type(data)}")
        app.logger.info(f"Keys: {list(data.keys()) if isinstance(data, dict) else 'Not a dict'}")
        
        if isinstance(data, dict):
            # Check for content
            content = data.get('content', [])
            app.logger.info(f"Content items: {len(content)}")
            
            # Check for images
            images = data.get('images', [])
            app.logger.info(f"Images: {len(images)}")
            
            # Sample first content item
            if content:
                first_item = content[0]
                app.logger.info(f"First content item keys: {list(first_item.keys())}")
                app.logger.info(f"First content page: {first_item.get('page')}")
                app.logger.info(f"First content has text: {'text' in first_item}")
                app.logger.info(f"First content has html: {'html_formatted_text' in first_item}")
        
        return jsonify({
            "status": "debug_complete",
            "data_type": str(type(data)),
            "keys": list(data.keys()) if isinstance(data, dict) else [],
            "content_count": len(data.get('content', [])) if isinstance(data, dict) else 0,
            "image_count": len(data.get('images', [])) if isinstance(data, dict) else 0
        }), 200
        
    except Exception as e:
        return jsonify({"error": str(e)}), 500

@app.route('/api/generate-flashcard-data', methods=['POST'])
def generate_flashcard_data():
    """Generate complete flashcard data structure for n8n"""
    try:
        data = request.get_json(force=True)
        
        # Process and organize the data
        organized_data = process_pdf_data(data)
        
        # Generate formatted content for LLM
        flashcard_content = []
        
        # Process all content types
        for content_type in ["critical", "highYield", "testable"]:
            for chunk in organized_data["content"][content_type]:
                flashcard_content.append({
                    "page": chunk["page"],
                    "html_formatted_text": chunk["html_formatted_text"],
                    "is_important": chunk["is_important"],
                    "is_critical_warning": chunk["is_critical_warning"],
                    "high_yield_score": chunk["high_yield_score"],
                    "formatting": chunk["formatting"],
                    "testable_facts": chunk["testable_facts"],
                    "medical_category": chunk["medical_category"],
                    "associated_images": chunk["images"]
                })
        
        # If no categorized content, process all content
        if not flashcard_content and "content" in data:
            for idx, chunk in enumerate(data.get("content", [])):
                page_num = chunk.get("page", 1)
                
                # Find associated images for this page
                associated_images = []
                for img_idx, img in enumerate(organized_data["images"]):
                    if img["page"] == page_num:
                        associated_images.append(img["ref"])
                
                flashcard_content.append({
                    "page": page_num,
                    "html_formatted_text": chunk.get("html_formatted_text", chunk.get("text", "")),
                    "is_important": chunk.get("is_important", False),
                    "is_critical_warning": chunk.get("is_critical_warning", False),
                    "high_yield_score": chunk.get("high_yield_score", 0),
                    "formatting": chunk.get("formatting", {}),
                    "testable_facts": chunk.get("testable_facts", []),
                    "medical_category": chunk.get("medical_category", "General"),
                    "associated_images": associated_images
                })
        
        # Create the complete output structure
        output = {
            "FLASHCARD_DATA": {
                "content": flashcard_content,
                "images": organized_data["images"],
                "session_objectives": organized_data["objectives"]
            },
            "AI_PROMPT": generate_flashcard_prompt(organized_data),
            "IMAGE_STORE": {
                "mainTopic": organized_data["metadata"]["mainTopic"],
                "uniformTag": organized_data["metadata"]["uniformTag"],
                "totalImages": len(organized_data["images"]),
                "imageMapping": organized_data["images"]
            }
        }
        
        return jsonify(output), 200
        
    except Exception as e:
        app.logger.error(f"Error generating flashcard data: {e}")
        import traceback
        return jsonify({
            "error": str(e),
            "traceback": traceback.format_exc()
        }), 500

# Keep all your existing endpoints and code below...